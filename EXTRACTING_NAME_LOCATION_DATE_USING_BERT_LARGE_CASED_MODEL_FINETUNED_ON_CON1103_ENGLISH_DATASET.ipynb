{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ceee1f7-4e2d-46d2-9614-314284d1e6c4",
   "metadata": {},
   "source": [
    "# EXTRACTING NAME, LOCATION, EMAIL,DATE FROM TEXT USING BERT LARGE CASED MODEL FINETUNED ON CON1103 ENGLISH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e52b6f7-c708-4e3c-9e92-9e1f61893338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd92167f-5ba1-46f4-a325-796e6cf1593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bc4597-aedd-428c-84c3-c3eb4c30cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52656bd-8181-4a60-9aba-b336255c85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"\"\"\n",
    "My name is kunal singh and i am persuing PG in artificial intelligence and machine learning from 23 sept 2023 to 23 august 2024 and I live in singhatiya \n",
    "and my email id is kunal.jcdu@gmail.com and my company name will be shiv tandav.pvt.ltd\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42503aed-9d23-4827-8ac0-be5e89cfca3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'k',\n",
       " '##una',\n",
       " '##l',\n",
       " 'sing',\n",
       " '##h',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'per',\n",
       " '##sui',\n",
       " '##ng',\n",
       " 'P',\n",
       " '##G',\n",
       " 'in',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'from',\n",
       " '23',\n",
       " 'se',\n",
       " '##pt',\n",
       " '202',\n",
       " '##3',\n",
       " 'to',\n",
       " '23',\n",
       " 'au',\n",
       " '##gus',\n",
       " '##t',\n",
       " '202',\n",
       " '##4',\n",
       " 'and',\n",
       " 'I',\n",
       " 'live',\n",
       " 'in',\n",
       " 'sing',\n",
       " '##hat',\n",
       " '##iya',\n",
       " 'and',\n",
       " 'my',\n",
       " 'email',\n",
       " 'id',\n",
       " 'is',\n",
       " 'k',\n",
       " '##una',\n",
       " '##l',\n",
       " '.',\n",
       " 'j',\n",
       " '##c',\n",
       " '##du',\n",
       " '@',\n",
       " 'g',\n",
       " '##mail',\n",
       " '.',\n",
       " 'com',\n",
       " 'and',\n",
       " 'my',\n",
       " 'company',\n",
       " 'name',\n",
       " 'will',\n",
       " 'be',\n",
       " 's',\n",
       " '##hi',\n",
       " '##v',\n",
       " 'tan',\n",
       " '##da',\n",
       " '##v',\n",
       " '.',\n",
       " 'p',\n",
       " '##v',\n",
       " '##t',\n",
       " '.',\n",
       " 'l',\n",
       " '##t',\n",
       " '##d']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_words=tokenizer.tokenize(text)\n",
    "token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e478383-f358-41a9-8730-063c58809a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1422,\n",
       " 1271,\n",
       " 1110,\n",
       " 180,\n",
       " 9291,\n",
       " 1233,\n",
       " 6928,\n",
       " 1324,\n",
       " 1105,\n",
       " 178,\n",
       " 1821,\n",
       " 1679,\n",
       " 26841,\n",
       " 2118,\n",
       " 153,\n",
       " 2349,\n",
       " 1107,\n",
       " 8246,\n",
       " 4810,\n",
       " 1105,\n",
       " 3395,\n",
       " 3776,\n",
       " 1121,\n",
       " 1695,\n",
       " 14516,\n",
       " 6451,\n",
       " 17881,\n",
       " 1495,\n",
       " 1106,\n",
       " 1695,\n",
       " 12686,\n",
       " 12909,\n",
       " 1204,\n",
       " 17881,\n",
       " 1527,\n",
       " 1105,\n",
       " 146,\n",
       " 1686,\n",
       " 1107,\n",
       " 6928,\n",
       " 11220,\n",
       " 9384,\n",
       " 1105,\n",
       " 1139,\n",
       " 10632,\n",
       " 25021,\n",
       " 1110,\n",
       " 180,\n",
       " 9291,\n",
       " 1233,\n",
       " 119,\n",
       " 179,\n",
       " 1665,\n",
       " 7641,\n",
       " 137,\n",
       " 176,\n",
       " 14746,\n",
       " 119,\n",
       " 3254,\n",
       " 1105,\n",
       " 1139,\n",
       " 1419,\n",
       " 1271,\n",
       " 1209,\n",
       " 1129,\n",
       " 188,\n",
       " 3031,\n",
       " 1964,\n",
       " 15925,\n",
       " 1810,\n",
       " 1964,\n",
       " 119,\n",
       " 185,\n",
       " 1964,\n",
       " 1204,\n",
       " 119,\n",
       " 181,\n",
       " 1204,\n",
       " 1181]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=tokenizer.convert_tokens_to_ids(token_words)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87548a28-5d6b-4e6f-8279-0a0efa19e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get named entities\n",
    "entities = ner_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa13569-04ac-4763-a460-bed5df7b7b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter entities by type\n",
    "def filter_entities(entities):\n",
    "    filtered_entities = {\n",
    "        \"PERSON\": [],\n",
    "        \"LOC\": [],\n",
    "        \"ORG\": [],\n",
    "        \"DATE\": [],\n",
    "        \"EMAIL\": []\n",
    "    }\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity'].split('-')[-1]\n",
    "        if entity_type in filtered_entities:\n",
    "            filtered_entities[entity_type].append(entity['word'])\n",
    "    \n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf77be3c-e2af-4096-977e-2bbac171cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_entities = filter_entities(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989b2b7c-e30f-4854-82e8-6a9ae6dfad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': [],\n",
       " 'LOC': ['sing', '##hat', '##iya'],\n",
       " 'ORG': [],\n",
       " 'DATE': [],\n",
       " 'EMAIL': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70afb62-e3af-4484-98cf-7394baff3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': [],\n",
       " 'LOC': ['sing', '##hat', '##iya'],\n",
       " 'ORG': [],\n",
       " 'DATE': [],\n",
       " 'EMAIL': ['kunal.jcdu@gmail.com']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract emails using regex\n",
    "import re\n",
    "\n",
    "def extract_emails(text):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    return emails\n",
    "\n",
    "emails = extract_emails(text)\n",
    "filtered_entities['EMAIL'] = emails\n",
    "\n",
    "filtered_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b798a-9c94-40d1-98bd-8dd784404f76",
   "metadata": {},
   "source": [
    "# LETS TRY ANOTHER LLM MODEL FOR EXTRACTING NAMED ENTITY RECOGNITION(NAMES,PHONE NUMBERS, EMAI IDS, SKILLS FROM A RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25997c3-5a99-4f91-b7f9-8a34bb9dff3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9868324,\n",
       "  'index': 4,\n",
       "  'word': 'k',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9746304,\n",
       "  'index': 5,\n",
       "  'word': '##una',\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9898369,\n",
       "  'index': 6,\n",
       "  'word': '##l',\n",
       "  'start': 15,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9933996,\n",
       "  'index': 7,\n",
       "  'word': 'sing',\n",
       "  'start': 17,\n",
       "  'end': 21},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9221179,\n",
       "  'index': 8,\n",
       "  'word': '##h',\n",
       "  'start': 21,\n",
       "  'end': 22},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.53480405,\n",
       "  'index': 38,\n",
       "  'word': 'sing',\n",
       "  'start': 142,\n",
       "  'end': 146},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.61715794,\n",
       "  'index': 39,\n",
       "  'word': '##hat',\n",
       "  'start': 146,\n",
       "  'end': 149},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.6104713,\n",
       "  'index': 40,\n",
       "  'word': '##iya',\n",
       "  'start': 149,\n",
       "  'end': 152}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Try a different NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is kunal singh and i am pursuing PG in artificial intelligence and machine learning from 23 sept 2023 to 23 august 2024 and I live in singhatiya\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "\n",
    "ner_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b511b8-d4a0-47ee-a4fe-ecfce63e9063",
   "metadata": {},
   "source": [
    "# LETS EXTRACT 4 THINGS FROM A ANOTHER MODEL(PERSON NAMES, DATES, LOCATIONS, AND MIISCELLANOUS) FROM A TEXT USING BERT-LARGE-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0404dd32-6cb8-4d70-83bc-e768ab9c08fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd131678cced40e8a7e34ef8715a1323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--dslim--bert-large-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e9308655134b5f9411cb79fe66de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290dea2d9fcf4dd19a0f27f4ade66513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c057090c7404ba28031aaa23d0140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a485f06974e46cc8b608ea76437299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c91fb0-7190-46ce-9f23-ba96d8cf3144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.98376864, 'index': 4, 'word': 'k', 'start': 11, 'end': 12}, {'entity': 'B-PER', 'score': 0.32892758, 'index': 5, 'word': '##una', 'start': 12, 'end': 15}, {'entity': 'I-PER', 'score': 0.7284298, 'index': 6, 'word': '##l', 'start': 15, 'end': 16}, {'entity': 'I-PER', 'score': 0.7760353, 'index': 7, 'word': 'sing', 'start': 17, 'end': 21}, {'entity': 'I-PER', 'score': 0.8695711, 'index': 8, 'word': '##h', 'start': 21, 'end': 22}, {'entity': 'B-LOC', 'score': 0.76172465, 'index': 40, 'word': 'sing', 'start': 142, 'end': 146}, {'entity': 'B-LOC', 'score': 0.5201816, 'index': 41, 'word': '##hat', 'start': 146, 'end': 149}, {'entity': 'I-LOC', 'score': 0.5219515, 'index': 42, 'word': '##iya', 'start': 149, 'end': 152}]\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "#from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is kunal singh and i am persuing PG in artificial intelligence and machine learning from 23 sept 2023 to 23 august 2024 and I live in singhatiya\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ef21d7-ce83-4938-9163-bc1053cf9b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.98376864,\n",
       "  'index': 4,\n",
       "  'word': 'k',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.32892758,\n",
       "  'index': 5,\n",
       "  'word': '##una',\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.7284298,\n",
       "  'index': 6,\n",
       "  'word': '##l',\n",
       "  'start': 15,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.7760353,\n",
       "  'index': 7,\n",
       "  'word': 'sing',\n",
       "  'start': 17,\n",
       "  'end': 21},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.8695711,\n",
       "  'index': 8,\n",
       "  'word': '##h',\n",
       "  'start': 21,\n",
       "  'end': 22},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.76172465,\n",
       "  'index': 40,\n",
       "  'word': 'sing',\n",
       "  'start': 142,\n",
       "  'end': 146},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.5201816,\n",
       "  'index': 41,\n",
       "  'word': '##hat',\n",
       "  'start': 146,\n",
       "  'end': 149},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.5219515,\n",
       "  'index': 42,\n",
       "  'word': '##iya',\n",
       "  'start': 149,\n",
       "  'end': 152}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f419e2f-d1b1-40bc-b5f6-edf43be5c885",
   "metadata": {},
   "source": [
    "# LETS TRY SOME DIFFERENT SENTENCE AND TRY TO EXTRACT NAMED ENTITIES FROM IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1750b4b0-3f93-41f6-966b-c3a7b420baeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n1 Speech to Speech Chatbot for Airplane flying school  |   LINK \\n \\n\\uf0be Performed Text Processing to create Question Answer pair Template  \\n\\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging \\nFace, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine \\nTuning(SFT) \\n\\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was,  \\nAirplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\n \\n\\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech \\ninto text and then passed the text to our customized chatbot. The chat gave response in text \\nform which I converted it into speech medium using Text to Speech technology. \\n. \\n \\n2  Object detection model  |  LINK \\n\\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\n\\uf0be Improvements done:  \\n1. Added audio to the output which is not present in output of yolov8. \\n2. Calculated distance between objects in the video frame. \\n3. Created circular bounding box instead of rectangular bounding box. \\nKUNAL KUMAR \\nDATA SCIENTIST \\n \\n \\nPhone No: 9060804108                 \\n \\nEmail: kunal.jcdu@gmail.com    \\n \\nLinkedin | ( Kunal Kumar)              \\n \\nGithub | (SolveKunal)                        \\n \\n SKILLS  SUMMARY \\n   WORK EXPERIENCE \\n \\n \\n \\n \\n \\n \\nDATA ANALYST INTERN  |   LINK \\n\\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%,   \\nenhancing efficiency. \\n\\uf0b7 Collaborated  with 3+ cross-functional teams to gather information to decide Project \\nscopes and ensure alignment with business objectives. \\n\\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends \\nand created Dashboard using power BI \\n   PROJECTS \\n \\nLanguages:              Python, SQL \\nFrameworks:            Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn,  \\nTools:                Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook,   .        \\n.                                 Speech to Text, Text to Speech, pyttsx3, Speech Recognizer                         \\nTechnical Skills:  NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object        \\ndetection, Image Classification, similar sentence search, article summarizer, Machine Learning, \\nsupervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data \\nModelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market \\nAnalysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis \\n',\n",
       " ' \\n 3   Pneumonia Detection    LINK  \\n\\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\n\\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model \\nparameters. \\n\\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras \\n           Tensorflow, VGG-16 \\n \\n 4   Live Dashboard creation \\n\\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then \\ntransferred the cleaned dataset to Power BI. \\n\\uf0be Identified 10+ major trends from the dataset which contributed to decision-making \\nprocess. \\n\\uf0be Interactive  Dashboard was built and the link was sent to the University Professor. \\n \\n5  Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\n\\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and \\nTransformers model.  \\n\\uf0be Then trained a Random-Forest Classifier with  Embeddings as inputs to predict Sentiment of \\neach movie review and do Similar Sentence Search in a supervised technique mode. \\n\\uf0be Using Transformer model I achieved  more similar sentence  searches as here \\nContext Aware Embeddings were used. \\n \\n6  Restaurant Revenue Prediction \\n\\uf0be This project was part of the hackathon organized by Great Learning platform.  \\n\\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random \\nForest Regressor ML model \\n\\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy.  \\n\\uf0be However, after the competition was over, I increased the models accuracy to 95.5% \\nusing XG-Boost model. \\n \\n \\n\\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning               Undergoing \\n           University of Texas at Austin \\n \\n\\uf0d8 B-Tech in Electronics and Communication  Engineering                             2014-2018 \\n           Vellore Institute of Technology (VIT Vellore) \\n   EDUCATION \\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "text=fitz.get_text(r'E:\\new_downloads_default_folder\\RESUME_AS_OF_17_JUNE.pdf')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada72016-d701-4779-87f7-fb87c408dcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text\n\u001b[0;32m      2\u001b[0m text2\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "text2=\" \" + text\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c3c50c9-75b0-467b-8392-4759f46c17e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n1 Speech to Speech Chatbot for Airplane flying school  |   LINK \\n \\n\\uf0be Performed Text Processing to create Question Answer pair Template  \\n\\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging \\nFace, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine \\nTuning(SFT) \\n\\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was,  \\nAirplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\n \\n\\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech \\ninto text and then passed the text to our customized chatbot. The chat gave response in text \\nform which I converted it into speech medium using Text to Speech technology. \\n. \\n \\n2  Object detection model  |  LINK \\n\\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\n\\uf0be Improvements done:  \\n1. Added audio to the output which is not present in output of yolov8. \\n2. Calculated distance between objects in the video frame. \\n3. Created circular bounding box instead of rectangular bounding box. \\nKUNAL KUMAR \\nDATA SCIENTIST \\n \\n \\nPhone No: 9060804108                 \\n \\nEmail: kunal.jcdu@gmail.com    \\n \\nLinkedin | ( Kunal Kumar)              \\n \\nGithub | (SolveKunal)                        \\n \\n SKILLS  SUMMARY \\n   WORK EXPERIENCE \\n \\n \\n \\n \\n \\n \\nDATA ANALYST INTERN  |   LINK \\n\\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%,   \\nenhancing efficiency. \\n\\uf0b7 Collaborated  with 3+ cross-functional teams to gather information to decide Project \\nscopes and ensure alignment with business objectives. \\n\\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends \\nand created Dashboard using power BI \\n   PROJECTS \\n \\nLanguages:              Python, SQL \\nFrameworks:            Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn,  \\nTools:                Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook,   .        \\n.                                 Speech to Text, Text to Speech, pyttsx3, Speech Recognizer                         \\nTechnical Skills:  NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object        \\ndetection, Image Classification, similar sentence search, article summarizer, Machine Learning, \\nsupervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data \\nModelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market \\nAnalysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis \\n  \\n 3   Pneumonia Detection    LINK  \\n\\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\n\\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model \\nparameters. \\n\\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras \\n           Tensorflow, VGG-16 \\n \\n 4   Live Dashboard creation \\n\\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then \\ntransferred the cleaned dataset to Power BI. \\n\\uf0be Identified 10+ major trends from the dataset which contributed to decision-making \\nprocess. \\n\\uf0be Interactive  Dashboard was built and the link was sent to the University Professor. \\n \\n5  Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\n\\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and \\nTransformers model.  \\n\\uf0be Then trained a Random-Forest Classifier with  Embeddings as inputs to predict Sentiment of \\neach movie review and do Similar Sentence Search in a supervised technique mode. \\n\\uf0be Using Transformer model I achieved  more similar sentence  searches as here \\nContext Aware Embeddings were used. \\n \\n6  Restaurant Revenue Prediction \\n\\uf0be This project was part of the hackathon organized by Great Learning platform.  \\n\\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random \\nForest Regressor ML model \\n\\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy.  \\n\\uf0be However, after the competition was over, I increased the models accuracy to 95.5% \\nusing XG-Boost model. \\n \\n \\n\\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning               Undergoing \\n           University of Texas at Austin \\n \\n\\uf0d8 B-Tech in Electronics and Communication  Engineering                             2014-2018 \\n           Vellore Institute of Technology (VIT Vellore) \\n   EDUCATION \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=\" \".join(text)                     # THIS IS HOW WE CONVERT LIST INTO A STRING \n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc317b69-18fb-43ec-aa53-88a6f077bdd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 Speech to Speech Chatbot for Airplane flying school | LINK \\uf0be Performed Text Processing to create Question Answer pair Template \\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging Face, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine Tuning(SFT) \\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was, Airplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech into text and then passed the text to our customized chatbot. The chat gave response in text form which I converted it into speech medium using Text to Speech technology. . 2 Object detection model | LINK \\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\uf0be Improvements done: 1. Added audio to the output which is not present in output of yolov8. 2. Calculated distance between objects in the video frame. 3. Created circular bounding box instead of rectangular bounding box. KUNAL KUMAR DATA SCIENTIST Phone No: 9060804108 Email: kunal.jcdu@gmail.com Linkedin | ( Kunal Kumar) Github | (SolveKunal) SKILLS SUMMARY WORK EXPERIENCE DATA ANALYST INTERN | LINK \\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%, enhancing efficiency. \\uf0b7 Collaborated with 3+ cross-functional teams to gather information to decide Project scopes and ensure alignment with business objectives. \\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends and created Dashboard using power BI PROJECTS Languages: Python, SQL Frameworks: Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn, Tools: Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook, . . Speech to Text, Text to Speech, pyttsx3, Speech Recognizer Technical Skills: NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text=re.sub(r'\\s+',' ',  text2)                   # re.sub(pattern,replacement,text)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2ac21-6a5a-4284-a42a-e3b7814cea04",
   "metadata": {},
   "source": [
    "# NOW, FEED THE EXTRACTED AND CLEANED TEXT OF THE RESUME PDF TO THE BERT MODEL TO EXTRACT NAMED ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74a131f6-a0a2-437d-b4c5-1761875d577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_resume = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model_resume = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56e41b26-31ac-4c04-a418-d39c67d96ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-MISC',\n",
       "  'score': 0.62508166,\n",
       "  'index': 2,\n",
       "  'word': 'Speech',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.96644306,\n",
       "  'index': 9,\n",
       "  'word': 'Air',\n",
       "  'start': 32,\n",
       "  'end': 35},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9739333,\n",
       "  'index': 10,\n",
       "  'word': '##plane',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.70077974,\n",
       "  'index': 14,\n",
       "  'word': 'L',\n",
       "  'start': 57,\n",
       "  'end': 58},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.94651216,\n",
       "  'index': 36,\n",
       "  'word': 'Transformers',\n",
       "  'start': 166,\n",
       "  'end': 178},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.99461454,\n",
       "  'index': 38,\n",
       "  'word': 'L',\n",
       "  'start': 180,\n",
       "  'end': 181},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9825911,\n",
       "  'index': 39,\n",
       "  'word': '##lam',\n",
       "  'start': 181,\n",
       "  'end': 184},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.98890775,\n",
       "  'index': 40,\n",
       "  'word': '##a',\n",
       "  'start': 184,\n",
       "  'end': 185},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.77167714,\n",
       "  'index': 41,\n",
       "  'word': '-',\n",
       "  'start': 185,\n",
       "  'end': 186},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.93122756,\n",
       "  'index': 42,\n",
       "  'word': '2',\n",
       "  'start': 186,\n",
       "  'end': 187},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.70214856,\n",
       "  'index': 44,\n",
       "  'word': '7',\n",
       "  'start': 188,\n",
       "  'end': 189},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.67606044,\n",
       "  'index': 47,\n",
       "  'word': 'NL',\n",
       "  'start': 192,\n",
       "  'end': 194},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.5464793,\n",
       "  'index': 48,\n",
       "  'word': '##P',\n",
       "  'start': 194,\n",
       "  'end': 195},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9004847,\n",
       "  'index': 50,\n",
       "  'word': 'P',\n",
       "  'start': 197,\n",
       "  'end': 198},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.7327628,\n",
       "  'index': 52,\n",
       "  'word': '##T',\n",
       "  'start': 199,\n",
       "  'end': 200},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.915159,\n",
       "  'index': 56,\n",
       "  'word': 'Hu',\n",
       "  'start': 206,\n",
       "  'end': 208},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.8138337,\n",
       "  'index': 58,\n",
       "  'word': 'Face',\n",
       "  'start': 214,\n",
       "  'end': 218},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.91683614,\n",
       "  'index': 60,\n",
       "  'word': 'Lo',\n",
       "  'start': 220,\n",
       "  'end': 222},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.91448134,\n",
       "  'index': 61,\n",
       "  'word': '##RA',\n",
       "  'start': 222,\n",
       "  'end': 224},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.91671425,\n",
       "  'index': 63,\n",
       "  'word': 'Q',\n",
       "  'start': 226,\n",
       "  'end': 227},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.78885484,\n",
       "  'index': 64,\n",
       "  'word': '##L',\n",
       "  'start': 227,\n",
       "  'end': 228},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.8488544,\n",
       "  'index': 66,\n",
       "  'word': '##RA',\n",
       "  'start': 229,\n",
       "  'end': 231},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.50159407,\n",
       "  'index': 68,\n",
       "  'word': 'Para',\n",
       "  'start': 233,\n",
       "  'end': 237},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9959571,\n",
       "  'index': 95,\n",
       "  'word': 'L',\n",
       "  'start': 322,\n",
       "  'end': 323},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9913161,\n",
       "  'index': 96,\n",
       "  'word': '##lam',\n",
       "  'start': 323,\n",
       "  'end': 326},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.99123305,\n",
       "  'index': 97,\n",
       "  'word': '##a',\n",
       "  'start': 326,\n",
       "  'end': 327},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.82986623,\n",
       "  'index': 98,\n",
       "  'word': '-',\n",
       "  'start': 327,\n",
       "  'end': 328},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9373359,\n",
       "  'index': 99,\n",
       "  'word': '2',\n",
       "  'start': 328,\n",
       "  'end': 329},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.55544204,\n",
       "  'index': 100,\n",
       "  'word': '-',\n",
       "  'start': 329,\n",
       "  'end': 330},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.7096823,\n",
       "  'index': 101,\n",
       "  'word': '7',\n",
       "  'start': 330,\n",
       "  'end': 331},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9964838,\n",
       "  'index': 115,\n",
       "  'word': 'Air',\n",
       "  'start': 386,\n",
       "  'end': 389},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9959906,\n",
       "  'index': 116,\n",
       "  'word': '##plane',\n",
       "  'start': 389,\n",
       "  'end': 394},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9934977,\n",
       "  'index': 117,\n",
       "  'word': 'Flying',\n",
       "  'start': 395,\n",
       "  'end': 401},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9973022,\n",
       "  'index': 118,\n",
       "  'word': 'Handbook',\n",
       "  'start': 402,\n",
       "  'end': 410},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.69537026,\n",
       "  'index': 120,\n",
       "  'word': 'US',\n",
       "  'start': 414,\n",
       "  'end': 416},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.87664354,\n",
       "  'index': 121,\n",
       "  'word': 'Federal',\n",
       "  'start': 417,\n",
       "  'end': 424},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.7747615,\n",
       "  'index': 122,\n",
       "  'word': 'Aviation',\n",
       "  'start': 425,\n",
       "  'end': 433},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.7074192,\n",
       "  'index': 123,\n",
       "  'word': 'Administration',\n",
       "  'start': 434,\n",
       "  'end': 448},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.77960485,\n",
       "  'index': 125,\n",
       "  'word': 'D',\n",
       "  'start': 449,\n",
       "  'end': 450},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.65426993,\n",
       "  'index': 126,\n",
       "  'word': '##OT',\n",
       "  'start': 450,\n",
       "  'end': 452},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97366726,\n",
       "  'index': 128,\n",
       "  'word': 'FAA',\n",
       "  'start': 453,\n",
       "  'end': 456},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.8334103,\n",
       "  'index': 142,\n",
       "  'word': 'Speech',\n",
       "  'start': 505,\n",
       "  'end': 511},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9686836,\n",
       "  'index': 178,\n",
       "  'word': 'Text',\n",
       "  'start': 693,\n",
       "  'end': 697},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.65070385,\n",
       "  'index': 179,\n",
       "  'word': 'to',\n",
       "  'start': 698,\n",
       "  'end': 700},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9917686,\n",
       "  'index': 180,\n",
       "  'word': 'Speech',\n",
       "  'start': 701,\n",
       "  'end': 707},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.69458175,\n",
       "  'index': 190,\n",
       "  'word': 'L',\n",
       "  'start': 749,\n",
       "  'end': 750},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.72033113,\n",
       "  'index': 195,\n",
       "  'word': 'Learning',\n",
       "  'start': 770,\n",
       "  'end': 778},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9756296,\n",
       "  'index': 201,\n",
       "  'word': 'Y',\n",
       "  'start': 797,\n",
       "  'end': 798},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9392924,\n",
       "  'index': 202,\n",
       "  'word': '##OL',\n",
       "  'start': 798,\n",
       "  'end': 800},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.90296197,\n",
       "  'index': 203,\n",
       "  'word': '##O',\n",
       "  'start': 800,\n",
       "  'end': 801},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.55206084,\n",
       "  'index': 205,\n",
       "  'word': '##8',\n",
       "  'start': 802,\n",
       "  'end': 803},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.57452565,\n",
       "  'index': 208,\n",
       "  'word': 'hugging',\n",
       "  'start': 815,\n",
       "  'end': 822},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.49032176,\n",
       "  'index': 261,\n",
       "  'word': 'K',\n",
       "  'start': 1049,\n",
       "  'end': 1050},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.67748904,\n",
       "  'index': 264,\n",
       "  'word': 'K',\n",
       "  'start': 1055,\n",
       "  'end': 1056},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99113584,\n",
       "  'index': 304,\n",
       "  'word': 'Ku',\n",
       "  'start': 1138,\n",
       "  'end': 1140},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.955184,\n",
       "  'index': 305,\n",
       "  'word': '##nal',\n",
       "  'start': 1140,\n",
       "  'end': 1143},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.996238,\n",
       "  'index': 306,\n",
       "  'word': 'Kumar',\n",
       "  'start': 1144,\n",
       "  'end': 1149},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.8964847,\n",
       "  'index': 308,\n",
       "  'word': 'G',\n",
       "  'start': 1151,\n",
       "  'end': 1152},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.5528361,\n",
       "  'index': 310,\n",
       "  'word': '##ub',\n",
       "  'start': 1155,\n",
       "  'end': 1157},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.8897905,\n",
       "  'index': 313,\n",
       "  'word': 'Sol',\n",
       "  'start': 1161,\n",
       "  'end': 1164},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.5528612,\n",
       "  'index': 314,\n",
       "  'word': '##ve',\n",
       "  'start': 1164,\n",
       "  'end': 1166},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.33541802,\n",
       "  'index': 315,\n",
       "  'word': '##K',\n",
       "  'start': 1166,\n",
       "  'end': 1167},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.6233671,\n",
       "  'index': 349,\n",
       "  'word': 'L',\n",
       "  'start': 1226,\n",
       "  'end': 1227},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.7771545,\n",
       "  'index': 396,\n",
       "  'word': 'Con',\n",
       "  'start': 1484,\n",
       "  'end': 1487},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9231342,\n",
       "  'index': 414,\n",
       "  'word': 'Dash',\n",
       "  'start': 1586,\n",
       "  'end': 1590},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.6660595,\n",
       "  'index': 418,\n",
       "  'word': 'B',\n",
       "  'start': 1608,\n",
       "  'end': 1609},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.63476866,\n",
       "  'index': 422,\n",
       "  'word': '##J',\n",
       "  'start': 1614,\n",
       "  'end': 1615},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9856694,\n",
       "  'index': 427,\n",
       "  'word': 'Python',\n",
       "  'start': 1631,\n",
       "  'end': 1637},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.7640953,\n",
       "  'index': 429,\n",
       "  'word': 'S',\n",
       "  'start': 1639,\n",
       "  'end': 1640},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9331518,\n",
       "  'index': 434,\n",
       "  'word': 'Mat',\n",
       "  'start': 1655,\n",
       "  'end': 1658},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9462567,\n",
       "  'index': 440,\n",
       "  'word': 'Sea',\n",
       "  'start': 1667,\n",
       "  'end': 1670},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.8057896,\n",
       "  'index': 443,\n",
       "  'word': 'NL',\n",
       "  'start': 1676,\n",
       "  'end': 1678},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.97068185,\n",
       "  'index': 447,\n",
       "  'word': 'Pan',\n",
       "  'start': 1682,\n",
       "  'end': 1685},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.92241096,\n",
       "  'index': 457,\n",
       "  'word': 'Lea',\n",
       "  'start': 1705,\n",
       "  'end': 1708},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.59332883,\n",
       "  'index': 464,\n",
       "  'word': 'B',\n",
       "  'start': 1725,\n",
       "  'end': 1726},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.5379014,\n",
       "  'index': 467,\n",
       "  'word': 'Table',\n",
       "  'start': 1729,\n",
       "  'end': 1734},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.9646042,\n",
       "  'index': 470,\n",
       "  'word': 'My',\n",
       "  'start': 1738,\n",
       "  'end': 1740},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.81741285,\n",
       "  'index': 477,\n",
       "  'word': 'Visual',\n",
       "  'start': 1752,\n",
       "  'end': 1758},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.8336891,\n",
       "  'index': 481,\n",
       "  'word': 'Ju',\n",
       "  'start': 1772,\n",
       "  'end': 1774},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.64100987,\n",
       "  'index': 489,\n",
       "  'word': 'Speech',\n",
       "  'start': 1794,\n",
       "  'end': 1800},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.5606488,\n",
       "  'index': 493,\n",
       "  'word': 'Text',\n",
       "  'start': 1810,\n",
       "  'end': 1814},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.67344177,\n",
       "  'index': 495,\n",
       "  'word': 'Speech',\n",
       "  'start': 1818,\n",
       "  'end': 1824},\n",
       " {'entity': 'B-MISC',\n",
       "  'score': 0.6126377,\n",
       "  'index': 503,\n",
       "  'word': 'Speech',\n",
       "  'start': 1835,\n",
       "  'end': 1841}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model_resume, tokenizer=tokenizer_resume)\n",
    "\n",
    "ner_results = nlp(cleaned_text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49a6c9-da15-4beb-9f0c-c8653c018a4c",
   "metadata": {},
   "source": [
    "# so to extract each value corrosponding to word stored in the dictionary which is stored in a list , you need to use LIST COMPHRENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce919d02-3c90-49ff-b80e-7bb7f0202c17",
   "metadata": {},
   "source": [
    "# LIST COMPHRESION TO EXTRACT VALUES FROM A LIST CONTAINING MANY DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2413bbc0-4f22-49ec-b09f-18b6ad438c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n",
      "Air\n",
      "##plane\n",
      "L\n",
      "Transformers\n",
      "L\n",
      "##lam\n",
      "##a\n",
      "-\n",
      "2\n",
      "7\n",
      "NL\n",
      "##P\n",
      "P\n",
      "##T\n",
      "Hu\n",
      "Face\n",
      "Lo\n",
      "##RA\n",
      "Q\n",
      "##L\n",
      "##RA\n",
      "Para\n",
      "L\n",
      "##lam\n",
      "##a\n",
      "-\n",
      "2\n",
      "-\n",
      "7\n",
      "Air\n",
      "##plane\n",
      "Flying\n",
      "Handbook\n",
      "US\n",
      "Federal\n",
      "Aviation\n",
      "Administration\n",
      "D\n",
      "##OT\n",
      "FAA\n",
      "Speech\n",
      "Text\n",
      "to\n",
      "Speech\n",
      "L\n",
      "Learning\n",
      "Y\n",
      "##OL\n",
      "##O\n",
      "##8\n",
      "hugging\n",
      "K\n",
      "K\n",
      "Ku\n",
      "##nal\n",
      "Kumar\n",
      "G\n",
      "##ub\n",
      "Sol\n",
      "##ve\n",
      "##K\n",
      "L\n",
      "Con\n",
      "Dash\n",
      "B\n",
      "##J\n",
      "Python\n",
      "S\n",
      "Mat\n",
      "Sea\n",
      "NL\n",
      "Pan\n",
      "Lea\n",
      "B\n",
      "Table\n",
      "My\n",
      "Visual\n",
      "Ju\n",
      "Speech\n",
      "Text\n",
      "Speech\n",
      "Speech\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Speech'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ner_results:                         # THIS CODE MEANS: GO INTO THIS LIST WHICH HAS MANY SUB DICTIONARIES IN IT. \n",
    "                                              # GO INSIDE EACH DICTIONARY ONE BY ONE AND extract the VALUE stored corrosponding to the key 'WORD'\n",
    "    print(i['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edaad64c-beb5-4d56-b2f6-1345a56338c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-MISC', 'score': 0.62508166, 'index': 2, 'word': 'Speech', 'start': 3, 'end': 9}, {'entity': 'B-MISC', 'score': 0.96644306, 'index': 9, 'word': 'Air', 'start': 32, 'end': 35}, {'entity': 'I-MISC', 'score': 0.9739333, 'index': 10, 'word': '##plane', 'start': 35, 'end': 40}, {'entity': 'B-ORG', 'score': 0.70077974, 'index': 14, 'word': 'L', 'start': 57, 'end': 58}, {'entity': 'B-MISC', 'score': 0.94651216, 'index': 36, 'word': 'Transformers', 'start': 166, 'end': 178}, {'entity': 'B-MISC', 'score': 0.99461454, 'index': 38, 'word': 'L', 'start': 180, 'end': 181}, {'entity': 'I-MISC', 'score': 0.9825911, 'index': 39, 'word': '##lam', 'start': 181, 'end': 184}, {'entity': 'I-MISC', 'score': 0.98890775, 'index': 40, 'word': '##a', 'start': 184, 'end': 185}, {'entity': 'I-MISC', 'score': 0.77167714, 'index': 41, 'word': '-', 'start': 185, 'end': 186}, {'entity': 'I-MISC', 'score': 0.93122756, 'index': 42, 'word': '2', 'start': 186, 'end': 187}, {'entity': 'I-MISC', 'score': 0.70214856, 'index': 44, 'word': '7', 'start': 188, 'end': 189}, {'entity': 'B-MISC', 'score': 0.67606044, 'index': 47, 'word': 'NL', 'start': 192, 'end': 194}, {'entity': 'I-MISC', 'score': 0.5464793, 'index': 48, 'word': '##P', 'start': 194, 'end': 195}, {'entity': 'B-MISC', 'score': 0.9004847, 'index': 50, 'word': 'P', 'start': 197, 'end': 198}, {'entity': 'I-MISC', 'score': 0.7327628, 'index': 52, 'word': '##T', 'start': 199, 'end': 200}, {'entity': 'B-MISC', 'score': 0.915159, 'index': 56, 'word': 'Hu', 'start': 206, 'end': 208}, {'entity': 'I-MISC', 'score': 0.8138337, 'index': 58, 'word': 'Face', 'start': 214, 'end': 218}, {'entity': 'B-MISC', 'score': 0.91683614, 'index': 60, 'word': 'Lo', 'start': 220, 'end': 222}, {'entity': 'I-MISC', 'score': 0.91448134, 'index': 61, 'word': '##RA', 'start': 222, 'end': 224}, {'entity': 'B-MISC', 'score': 0.91671425, 'index': 63, 'word': 'Q', 'start': 226, 'end': 227}, {'entity': 'I-MISC', 'score': 0.78885484, 'index': 64, 'word': '##L', 'start': 227, 'end': 228}, {'entity': 'I-MISC', 'score': 0.8488544, 'index': 66, 'word': '##RA', 'start': 229, 'end': 231}, {'entity': 'B-MISC', 'score': 0.50159407, 'index': 68, 'word': 'Para', 'start': 233, 'end': 237}, {'entity': 'B-MISC', 'score': 0.9959571, 'index': 95, 'word': 'L', 'start': 322, 'end': 323}, {'entity': 'I-MISC', 'score': 0.9913161, 'index': 96, 'word': '##lam', 'start': 323, 'end': 326}, {'entity': 'I-MISC', 'score': 0.99123305, 'index': 97, 'word': '##a', 'start': 326, 'end': 327}, {'entity': 'I-MISC', 'score': 0.82986623, 'index': 98, 'word': '-', 'start': 327, 'end': 328}, {'entity': 'I-MISC', 'score': 0.9373359, 'index': 99, 'word': '2', 'start': 328, 'end': 329}, {'entity': 'I-MISC', 'score': 0.55544204, 'index': 100, 'word': '-', 'start': 329, 'end': 330}, {'entity': 'I-MISC', 'score': 0.7096823, 'index': 101, 'word': '7', 'start': 330, 'end': 331}, {'entity': 'B-MISC', 'score': 0.9964838, 'index': 115, 'word': 'Air', 'start': 386, 'end': 389}, {'entity': 'I-MISC', 'score': 0.9959906, 'index': 116, 'word': '##plane', 'start': 389, 'end': 394}, {'entity': 'I-MISC', 'score': 0.9934977, 'index': 117, 'word': 'Flying', 'start': 395, 'end': 401}, {'entity': 'I-MISC', 'score': 0.9973022, 'index': 118, 'word': 'Handbook', 'start': 402, 'end': 410}, {'entity': 'I-MISC', 'score': 0.69537026, 'index': 120, 'word': 'US', 'start': 414, 'end': 416}, {'entity': 'I-ORG', 'score': 0.87664354, 'index': 121, 'word': 'Federal', 'start': 417, 'end': 424}, {'entity': 'I-ORG', 'score': 0.7747615, 'index': 122, 'word': 'Aviation', 'start': 425, 'end': 433}, {'entity': 'I-ORG', 'score': 0.7074192, 'index': 123, 'word': 'Administration', 'start': 434, 'end': 448}, {'entity': 'B-ORG', 'score': 0.77960485, 'index': 125, 'word': 'D', 'start': 449, 'end': 450}, {'entity': 'I-ORG', 'score': 0.65426993, 'index': 126, 'word': '##OT', 'start': 450, 'end': 452}, {'entity': 'I-ORG', 'score': 0.97366726, 'index': 128, 'word': 'FAA', 'start': 453, 'end': 456}, {'entity': 'B-MISC', 'score': 0.8334103, 'index': 142, 'word': 'Speech', 'start': 505, 'end': 511}, {'entity': 'B-MISC', 'score': 0.9686836, 'index': 178, 'word': 'Text', 'start': 693, 'end': 697}, {'entity': 'I-MISC', 'score': 0.65070385, 'index': 179, 'word': 'to', 'start': 698, 'end': 700}, {'entity': 'I-MISC', 'score': 0.9917686, 'index': 180, 'word': 'Speech', 'start': 701, 'end': 707}, {'entity': 'B-ORG', 'score': 0.69458175, 'index': 190, 'word': 'L', 'start': 749, 'end': 750}, {'entity': 'I-MISC', 'score': 0.72033113, 'index': 195, 'word': 'Learning', 'start': 770, 'end': 778}, {'entity': 'B-MISC', 'score': 0.9756296, 'index': 201, 'word': 'Y', 'start': 797, 'end': 798}, {'entity': 'I-MISC', 'score': 0.9392924, 'index': 202, 'word': '##OL', 'start': 798, 'end': 800}, {'entity': 'I-MISC', 'score': 0.90296197, 'index': 203, 'word': '##O', 'start': 800, 'end': 801}, {'entity': 'I-MISC', 'score': 0.55206084, 'index': 205, 'word': '##8', 'start': 802, 'end': 803}, {'entity': 'B-MISC', 'score': 0.57452565, 'index': 208, 'word': 'hugging', 'start': 815, 'end': 822}, {'entity': 'B-ORG', 'score': 0.49032176, 'index': 261, 'word': 'K', 'start': 1049, 'end': 1050}, {'entity': 'I-ORG', 'score': 0.67748904, 'index': 264, 'word': 'K', 'start': 1055, 'end': 1056}, {'entity': 'B-PER', 'score': 0.99113584, 'index': 304, 'word': 'Ku', 'start': 1138, 'end': 1140}, {'entity': 'B-PER', 'score': 0.955184, 'index': 305, 'word': '##nal', 'start': 1140, 'end': 1143}, {'entity': 'I-PER', 'score': 0.996238, 'index': 306, 'word': 'Kumar', 'start': 1144, 'end': 1149}, {'entity': 'B-MISC', 'score': 0.8964847, 'index': 308, 'word': 'G', 'start': 1151, 'end': 1152}, {'entity': 'I-MISC', 'score': 0.5528361, 'index': 310, 'word': '##ub', 'start': 1155, 'end': 1157}, {'entity': 'B-PER', 'score': 0.8897905, 'index': 313, 'word': 'Sol', 'start': 1161, 'end': 1164}, {'entity': 'B-PER', 'score': 0.5528612, 'index': 314, 'word': '##ve', 'start': 1164, 'end': 1166}, {'entity': 'I-PER', 'score': 0.33541802, 'index': 315, 'word': '##K', 'start': 1166, 'end': 1167}, {'entity': 'B-ORG', 'score': 0.6233671, 'index': 349, 'word': 'L', 'start': 1226, 'end': 1227}, {'entity': 'B-MISC', 'score': 0.7771545, 'index': 396, 'word': 'Con', 'start': 1484, 'end': 1487}, {'entity': 'B-MISC', 'score': 0.9231342, 'index': 414, 'word': 'Dash', 'start': 1586, 'end': 1590}, {'entity': 'B-MISC', 'score': 0.6660595, 'index': 418, 'word': 'B', 'start': 1608, 'end': 1609}, {'entity': 'I-MISC', 'score': 0.63476866, 'index': 422, 'word': '##J', 'start': 1614, 'end': 1615}, {'entity': 'B-MISC', 'score': 0.9856694, 'index': 427, 'word': 'Python', 'start': 1631, 'end': 1637}, {'entity': 'B-MISC', 'score': 0.7640953, 'index': 429, 'word': 'S', 'start': 1639, 'end': 1640}, {'entity': 'B-MISC', 'score': 0.9331518, 'index': 434, 'word': 'Mat', 'start': 1655, 'end': 1658}, {'entity': 'B-MISC', 'score': 0.9462567, 'index': 440, 'word': 'Sea', 'start': 1667, 'end': 1670}, {'entity': 'B-MISC', 'score': 0.8057896, 'index': 443, 'word': 'NL', 'start': 1676, 'end': 1678}, {'entity': 'B-MISC', 'score': 0.97068185, 'index': 447, 'word': 'Pan', 'start': 1682, 'end': 1685}, {'entity': 'B-MISC', 'score': 0.92241096, 'index': 457, 'word': 'Lea', 'start': 1705, 'end': 1708}, {'entity': 'B-MISC', 'score': 0.59332883, 'index': 464, 'word': 'B', 'start': 1725, 'end': 1726}, {'entity': 'B-MISC', 'score': 0.5379014, 'index': 467, 'word': 'Table', 'start': 1729, 'end': 1734}, {'entity': 'B-MISC', 'score': 0.9646042, 'index': 470, 'word': 'My', 'start': 1738, 'end': 1740}, {'entity': 'B-MISC', 'score': 0.81741285, 'index': 477, 'word': 'Visual', 'start': 1752, 'end': 1758}, {'entity': 'B-MISC', 'score': 0.8336891, 'index': 481, 'word': 'Ju', 'start': 1772, 'end': 1774}, {'entity': 'B-MISC', 'score': 0.64100987, 'index': 489, 'word': 'Speech', 'start': 1794, 'end': 1800}, {'entity': 'B-MISC', 'score': 0.5606488, 'index': 493, 'word': 'Text', 'start': 1810, 'end': 1814}, {'entity': 'B-MISC', 'score': 0.67344177, 'index': 495, 'word': 'Speech', 'start': 1818, 'end': 1824}, {'entity': 'B-MISC', 'score': 0.6126377, 'index': 503, 'word': 'Speech', 'start': 1835, 'end': 1841}]\n"
     ]
    }
   ],
   "source": [
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e149c51-1f63-4746-915a-b2360a0bb3d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ner_results:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ner_results([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "ner_results=' '.join(ner_results)\n",
    "\n",
    "for i in ner_results:\n",
    "    print(ner_results(['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bd708f3-9deb-46f3-9ef4-1beb42fda82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Air ##plane L Transformers L ##lam ##a - 2 7 NL ##P P ##T Hu Face Lo ##RA Q ##L ##RA Para L ##lam ##a - 2 - 7 Air ##plane Flying Handbook US Federal Aviation Administration D ##OT FAA Speech Text to Speech L Learning Y ##OL ##O ##8 hugging K K Ku ##nal Kumar G ##ub Sol ##ve ##K L Con Dash B ##J Python S Mat Sea NL Pan Lea B Table My Visual Ju Speech Text Speech Speech\n"
     ]
    }
   ],
   "source": [
    "# Extract words from each dictionary\n",
    "words = [entry['word'] for entry in ner_results]\n",
    "\n",
    "# Convert list of words to a single string with space delimiter\n",
    "words_string = \" \".join(words)\n",
    "\n",
    "print(words_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4f36c-eb83-48a8-9ebf-918d9351216c",
   "metadata": {},
   "source": [
    "# HOW TO CONVERT DICTIONARY INTO LIST # woww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a2b2dad-37b3-4394-870f-05ac76d1a1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'kunal', 'age': 28}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1={'name':'kunal','age':28 }\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2e08af1-0b3a-4958-9269-12eaca89de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=list(dict1.keys())\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f275be1-b7b6-423a-ae81-f16de9c5ada8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kunal', 28]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2=list(dict1.values())\n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e71b08-d824-45db-b732-525ef7277d04",
   "metadata": {},
   "source": [
    "# LETS TRY WITH 3RD NER MODEL (huggingface/bert-base-cased-finetuned-mrpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "435af7b4-5dd6-458a-bbb5-1c31e92fdc41",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "huggingface/bert-base-cased-finetuned-mrpc is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/huggingface/bert-base-cased-finetuned-mrpc/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    403\u001b[0m         path_or_repo_id,\n\u001b[0;32m    404\u001b[0m         filename,\n\u001b[0;32m    405\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    406\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    407\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    408\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    409\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    410\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    411\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    412\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    413\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    414\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1237\u001b[0m     )\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1325\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1646\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1647\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1648\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1649\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1650\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1651\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1652\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1653\u001b[0m )\n\u001b[0;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    373\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    374\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    375\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 396\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m     )\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6698db09-0bfd2ddf3bd8e9fa0c4239d1;a60fab33-a795-487f-b3e3-ea71684734ab)\n\nRepository Not Found for url: https://huggingface.co/huggingface/bert-base-cased-finetuned-mrpc/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface/bert-base-cased-finetuned-mrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:826\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    828\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:658\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    657\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 658\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    659\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    660\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    661\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    662\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    663\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    664\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    665\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    666\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    667\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    668\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    669\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    670\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    671\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    672\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\new_downloads_default_folder\\anaconda_destination_folder\\Lib\\site-packages\\transformers\\utils\\hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: huggingface/bert-base-cased-finetuned-mrpc is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"huggingface/bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864218b9-3a0f-47a0-b26f-2c74337ba17e",
   "metadata": {},
   "source": [
    "# THIS IS SHOWING ERRORBECAUSE, THIS MODEL IS NOT FINE TUNED FOR NAMED ENTITY RECOGNITION."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
